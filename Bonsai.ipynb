{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"BonsaiNotebook.ipynb","version":"0.3.2","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"RGFRB9ZVHWDY","colab_type":"code","colab":{}},"source":["# import helpermethods\n","import tensorflow as tf\n","import numpy as np\n","import sys\n","import os\n","import pandas as pd\n","sys.path.insert(0, '../../')\n","\n","#Provide the GPU number to be used\n","os.environ['CUDA_VISIBLE_DEVICES'] ='-1'\n","\n","#Bonsai imports\n","# from edgeml.trainer.bonsaiTrainer import BonsaiTrainer\n","# from edgeml.graph.bonsai import Bonsai\n","\n","# Fixing seeds for reproducibility\n","tf.set_random_seed(42)\n","np.random.seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rg1vqMfVHvls","colab_type":"text"},"source":["# Helper methods"]},{"cell_type":"code","metadata":{"id":"caCzClmoHthe","colab_type":"code","colab":{}},"source":["import argparse\n","import datetime\n","import os\n","import numpy as np\n","\n","\n","def checkIntPos(value):\n","    ivalue = int(value)\n","    if ivalue <= 0:\n","        raise argparse.ArgumentTypeError(\n","            \"%s is an invalid positive int value\" % value)\n","    return ivalue\n","\n","\n","def checkIntNneg(value):\n","    ivalue = int(value)\n","    if ivalue < 0:\n","        raise argparse.ArgumentTypeError(\n","            \"%s is an invalid non-neg int value\" % value)\n","    return ivalue\n","\n","\n","def checkFloatNneg(value):\n","    fvalue = float(value)\n","    if fvalue < 0:\n","        raise argparse.ArgumentTypeError(\n","            \"%s is an invalid non-neg float value\" % value)\n","    return fvalue\n","\n","\n","def checkFloatPos(value):\n","    fvalue = float(value)\n","    if fvalue <= 0:\n","        raise argparse.ArgumentTypeError(\n","            \"%s is an invalid positive float value\" % value)\n","    return fvalue\n","\n","\n","def str2bool(v):\n","    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise argparse.ArgumentTypeError('Boolean value expected.')\n","\n","\n","def getArgs():\n","    '''\n","    Function to parse arguments for Bonsai Algorithm\n","    '''\n","    parser = argparse.ArgumentParser(\n","        description='HyperParams for Bonsai Algorithm')\n","    parser.add_argument('-dir', '--data-dir', required=True,\n","                        help='Data directory containing' +\n","                        'train.npy and test.npy')\n","\n","    parser.add_argument('-d', '--depth', type=checkIntNneg, default=2,\n","                        help='Depth of Bonsai Tree ' +\n","                        '(default: 2 try: [0, 1, 3])')\n","    parser.add_argument('-p', '--proj-dim', type=checkIntPos, default=10,\n","                        help='Projection Dimension ' +\n","                        '(default: 20 try: [5, 10, 30])')\n","    parser.add_argument('-s', '--sigma', type=float, default=1.0,\n","                        help='Parameter for sigmoid sharpness ' +\n","                        '(default: 1.0 try: [3.0, 0.05, 0.1]')\n","    parser.add_argument('-e', '--epochs', type=checkIntPos, default=42,\n","                        help='Total Epochs (default: 42 try:[100, 150, 60])')\n","    parser.add_argument('-b', '--batch-size', type=checkIntPos,\n","                        help='Batch Size to be used ' +\n","                        '(default: max(100, sqrt(train_samples)))')\n","    parser.add_argument('-lr', '--learning-rate', type=checkFloatPos,\n","                        default=0.01, help='Initial Learning rate for ' +\n","                        'Adam Optimizer (default: 0.01)')\n","\n","    parser.add_argument('-rW', type=float, default=0.0001,\n","                        help='Regularizer for predictor parameter W  ' +\n","                        '(default: 0.0001 try: [0.01, 0.001, 0.00001])')\n","    parser.add_argument('-rV', type=float, default=0.0001,\n","                        help='Regularizer for predictor parameter V  ' +\n","                        '(default: 0.0001 try: [0.01, 0.001, 0.00001])')\n","    parser.add_argument('-rT', type=float, default=0.0001,\n","                        help='Regularizer for branching parameter Theta  ' +\n","                        '(default: 0.0001 try: [0.01, 0.001, 0.00001])')\n","    parser.add_argument('-rZ', type=float, default=0.00001,\n","                        help='Regularizer for projection parameter Z  ' +\n","                        '(default: 0.00001 try: [0.001, 0.0001, 0.000001])')\n","\n","    parser.add_argument('-sW', type=checkFloatPos,\n","                        help='Sparsity for predictor parameter W  ' +\n","                        '(default: For Binary classification 1.0 else 0.2 ' +\n","                        'try: [0.1, 0.3, 0.5])')\n","    parser.add_argument('-sV', type=checkFloatPos,\n","                        help='Sparsity for predictor parameter V  ' +\n","                        '(default: For Binary classification 1.0 else 0.2 ' +\n","                        'try: [0.1, 0.3, 0.5])')\n","    parser.add_argument('-sT', type=checkFloatPos,\n","                        help='Sparsity for branching parameter Theta  ' +\n","                        '(default: For Binary classification 1.0 else 0.2 ' +\n","                        'try: [0.1, 0.3, 0.5])')\n","    parser.add_argument('-sZ', type=checkFloatPos, default=0.2,\n","                        help='Sparsity for projection parameter Z  ' +\n","                        '(default: 0.2 try: [0.1, 0.3, 0.5])')\n","    parser.add_argument('-oF', '--output-file', default=None,\n","                        help='Output file for dumping the program output, ' +\n","                        '(default: stdout)')\n","\n","    parser.add_argument('-regression', type=str2bool, default=False,\n","                        help='boolean argument which controls whether to perform ' +\n","                        'regression or classification.' +\n","                        'default : False (Classification) values: [True, False]')\n","\n","    return parser.parse_args()\n","\n","\n","def getQuantArgs():\n","    '''\n","    Function to parse arguments for Model Quantisation\n","    '''\n","    parser = argparse.ArgumentParser(\n","        description='Arguments for quantizing Fast models. ' +\n","        'Works only for piece-wise linear non-linearities, ' +\n","        'like relu, quantTanh, quantSigm (check rnn.py for the definitions)')\n","    parser.add_argument('-dir', '--model-dir', required=True,\n","                        help='model directory containing' +\n","                        '*.npy weight files dumped from the trained model')\n","    parser.add_argument('-m', '--max-val', type=checkIntNneg, default=127,\n","                        help='this represents the maximum possible value ' +\n","                        'in model, essentially the byte complexity, ' +\n","                        '127=> 1 byte is default')\n","\n","    return parser.parse_args()\n","\n","\n","def createTimeStampDir(dataDir):\n","    '''\n","    Creates a Directory with timestamp as it's name\n","    '''\n","    if os.path.isdir(dataDir + '/TFBonsaiResults') is False:\n","        try:\n","            os.mkdir(dataDir + '/TFBonsaiResults')\n","        except OSError:\n","            print(\"Creation of the directory %s failed\" %\n","                  dataDir + '/TFBonsaiResults')\n","\n","    currDir = 'TFBonsaiResults/' + datetime.datetime.now().strftime(\"%H_%M_%S_%d_%m_%y\")\n","    if os.path.isdir(dataDir + '/' + currDir) is False:\n","        try:\n","            os.mkdir(dataDir + '/' + currDir)\n","        except OSError:\n","            print(\"Creation of the directory %s failed\" %\n","                  dataDir + '/' + currDir)\n","        else:\n","            return (dataDir + '/' + currDir)\n","    return None\n","\n","\n","def preProcessData(dataDir, w,isRegression=False):\n","    '''\n","    Function to pre-process input data\n","    Expects a .npy file of form [lbl feats] for each datapoint\n","    Outputs a train and test set datapoints appended with 1 for Bias induction\n","    dataDimension, numClasses are inferred directly\n","    '''\n","    train = np.load(dataDir + '/train_'+str(w)+'.npy')\n","    test = np.load(dataDir + '/test_'+str(w)+'.npy')\n","\n","    dataDimension = int(train.shape[1]) - 1\n","\n","    Xtrain = train[:, 1:dataDimension + 1]\n","    Ytrain_ = train[:, 0]\n","\n","    Xtest = test[:, 1:dataDimension + 1]\n","    Ytest_ = test[:, 0]\n","\n","    # Mean Var Normalisation\n","    mean = np.mean(Xtrain, 0)\n","    std = np.std(Xtrain, 0)\n","    std[std[:] < 0.000001] = 1\n","    Xtrain = (Xtrain - mean) / std\n","    Xtest = (Xtest - mean) / std\n","    # End Mean Var normalisation\n","\n","    # Classification.\n","    if (isRegression == False):\n","        numClasses = max(Ytrain_) - min(Ytrain_) + 1\n","        numClasses = int(max(numClasses, max(Ytest_) - min(Ytest_) + 1))\n","\n","        lab = Ytrain_.astype('uint8')\n","        lab = np.array(lab) - min(lab)\n","\n","        lab_ = np.zeros((Xtrain.shape[0], numClasses))\n","        lab_[np.arange(Xtrain.shape[0]), lab] = 1\n","        if (numClasses == 2):\n","            Ytrain = np.reshape(lab, [-1, 1])\n","        else:\n","            Ytrain = lab_\n","\n","        lab = Ytest_.astype('uint8')\n","        lab = np.array(lab) - min(lab)\n","\n","        lab_ = np.zeros((Xtest.shape[0], numClasses))\n","        lab_[np.arange(Xtest.shape[0]), lab] = 1\n","        if (numClasses == 2):\n","            Ytest = np.reshape(lab, [-1, 1])\n","        else:\n","            Ytest = lab_\n","\n","    elif (isRegression == True):\n","        # The number of classes is always 1, for regression.\n","        numClasses = 1\n","        Ytrain = Ytrain_\n","        Ytest = Ytest_\n","\n","    trainBias = np.ones([Xtrain.shape[0], 1])\n","    Xtrain = np.append(Xtrain, trainBias, axis=1)\n","    testBias = np.ones([Xtest.shape[0], 1])\n","    Xtest = np.append(Xtest, testBias, axis=1)\n","\n","    mean = np.append(mean, np.array([0]))\n","    std = np.append(std, np.array([1]))\n","    \n","    if (isRegression == False):\n","        return dataDimension + 1, numClasses, Xtrain, Ytrain, Xtest, Ytest, mean, std\n","    elif (isRegression == True):\n","        return dataDimension + 1, numClasses, Xtrain, Ytrain.reshape((-1, 1)), Xtest, Ytest.reshape((-1, 1)), mean, std\n","\n","\n","def dumpCommand(list, currDir):\n","    '''\n","    Dumps the current command to a file for further use\n","    '''\n","    commandFile = open(currDir + '/command.txt', 'w')\n","    command = \"python\"\n","\n","    command = command + \" \" + ' '.join(list)\n","    commandFile.write(command)\n","\n","    commandFile.flush()\n","    commandFile.close()\n","\n","\n","def saveMeanStd(mean, std, currDir):\n","    '''\n","    Function to save Mean and Std vectors\n","    '''\n","    np.save(currDir + '/mean.npy', mean)\n","    np.save(currDir + '/std.npy', std)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LjQNpiM_H95f","colab_type":"text"},"source":["#utils"]},{"cell_type":"code","metadata":{"id":"CNGyxDb6H1uB","colab_type":"code","colab":{}},"source":["#utils\n","import scipy.cluster\n","import scipy.spatial\n","import os\n","\n","\n","def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n","    '''\n","    This method can be used to estimate gamma for ProtoNN. An approximation to\n","    median heuristic is used here.\n","    1. First the data is collapsed into the projectionDimension by W_init. If\n","    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n","    data normalization is essential.\n","    2. Prototype are computed by running a  k-means clustering on the projected\n","    data.\n","    3. The median distance is then estimated by calculating median distance\n","    between prototypes and projected data points.\n","\n","    data needs to be [-1, numFeats]\n","    If using this method to initialize gamma, please use the W and B as well.\n","\n","    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n","    andand labels\n","\n","    TODO: Clustering fails due to singularity error if projecting upwards\n","\n","    W [dxd_cap]\n","    B [d_cap, m]\n","    returns gamma, W, B\n","    '''\n","    assert data.ndim == 2\n","    X = data\n","    featDim = data.shape[1]\n","    if projectionDimension > featDim:\n","        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n","        print(\"\\t estimation due to median heuristic could fail.\")\n","        print(\"\\tTo retain the projection dataDimension, provide\")\n","        print(\"\\ta value for gamma.\")\n","\n","    if W_init is None:\n","        W_init = np.random.normal(size=[featDim, projectionDimension])\n","    W = W_init\n","    XW = np.matmul(X, W)\n","    assert XW.shape[1] == projectionDimension\n","    assert XW.shape[0] == len(X)\n","    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n","    # the number of centroids m. Returns, [n x d_cap] centroids and\n","    # elementwise center information.\n","    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n","    # Requires two matrices. Number of observations x dimension of observation\n","    # space. Distances[i,j] is the distance between XW[i] and B[j]\n","    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n","    distances = np.reshape(distances, [-1])\n","    gamma = np.median(distances)\n","    gamma = 1 / (2.5 * gamma)\n","    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n","\n","\n","def multiClassHingeLoss(logits, label, batch_th):\n","    '''\n","    MultiClassHingeLoss to match C++ Version - No TF internal version\n","    '''\n","    flatLogits = tf.reshape(logits, [-1, ])\n","    label_ = tf.argmax(label, 1)\n","\n","    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n","    correctLogit = tf.gather(flatLogits, correctId)\n","\n","    maxLabel = tf.argmax(logits, 1)\n","    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n","\n","    wrongMaxLogit = tf.where(\n","        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n","\n","    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n","\n","\n","def crossEntropyLoss(logits, label):\n","    '''\n","    Cross Entropy loss for MultiClass case in joint training for\n","    faster convergence\n","    '''\n","    return tf.reduce_mean(\n","        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n","                                                   labels=tf.stop_gradient(label)))\n","\n","\n","def mean_absolute_error(logits, label):\n","    '''\n","    Function to compute the mean absolute error.\n","    '''\n","    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n","\n","\n","def hardThreshold(A, s):\n","    '''\n","    Hard thresholding function on Tensor A with sparsity s\n","    '''\n","    A_ = np.copy(A)\n","    A_ = A_.ravel()\n","    if len(A_) > 0:\n","        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n","        A_[np.abs(A_) < th] = 0.0\n","    A_ = A_.reshape(A.shape)\n","    return A_\n","\n","\n","def copySupport(src, dest):\n","    '''\n","    copy support of src tensor to dest tensor\n","    '''\n","    support = np.nonzero(src)\n","    dest_ = dest\n","    dest = np.zeros(dest_.shape)\n","    dest[support] = dest_[support]\n","    return dest\n","\n","\n","def countnnZ(A, s, bytesPerVar=4):\n","    '''\n","    Returns # of non-zeros and representative size of the tensor\n","    Uses dense for s >= 0.5 - 4 byte\n","    Else uses sparse - 8 byte\n","    '''\n","    params = 1\n","    hasSparse = False\n","    for i in range(0, len(A.shape)):\n","        params *= int(A.shape[i])\n","    if s < 0.5:\n","        nnZ = np.ceil(params * s)\n","        hasSparse = True\n","        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n","    else:\n","        nnZ = params\n","        return nnZ, nnZ * bytesPerVar, hasSparse\n","\n","\n","def getConfusionMatrix(predicted, target, numClasses):\n","    '''\n","    Returns a confusion matrix for a multiclass classification\n","    problem. `predicted` is a 1-D array of integers representing\n","    the predicted classes and `target` is the target classes.\n","\n","    confusion[i][j]: Number of elements of class j\n","        predicted as class i\n","    Labels are assumed to be in range(0, numClasses)\n","    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n","    in a user friendly form.\n","    '''\n","    assert(predicted.ndim == 1)\n","    assert(target.ndim == 1)\n","    arr = np.zeros([numClasses, numClasses])\n","\n","    for i in range(len(predicted)):\n","        arr[predicted[i]][target[i]] += 1\n","    return arr\n","\n","\n","def printFormattedConfusionMatrix(matrix):\n","    '''\n","    Given a 2D confusion matrix, prints it in a human readable way.\n","    The confusion matrix is expected to be a 2D numpy array with\n","    square dimensions\n","    '''\n","    assert(matrix.ndim == 2)\n","    assert(matrix.shape[0] == matrix.shape[1])\n","    RECALL = 'Recall'\n","    PRECISION = 'PRECISION'\n","    print(\"|%s|\" % ('True->'), end='')\n","    for i in range(matrix.shape[0]):\n","        print(\"%7d|\" % i, end='')\n","    print(\"%s|\" % 'Precision')\n","\n","    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n","    for i in range(matrix.shape[0]):\n","        print(\"%s|\" % ('-' * 7), end='')\n","    print(\"%s|\" % ('-' * len(PRECISION)))\n","\n","    precisionlist = np.sum(matrix, axis=1)\n","    recalllist = np.sum(matrix, axis=0)\n","    precisionlist = [matrix[i][i] / x if x !=\n","                     0 else -1 for i, x in enumerate(precisionlist)]\n","    recalllist = [matrix[i][i] / x if x !=\n","                  0 else -1 for i, x in enumerate(recalllist)]\n","    for i in range(matrix.shape[0]):\n","        # len recall = 6\n","        print(\"|%6d|\" % (i), end='')\n","        for j in range(matrix.shape[0]):\n","            print(\"%7d|\" % (matrix[i][j]), end='')\n","        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n","        if precisionlist[i] != -1:\n","            print(\"%1.5f|\" % precisionlist[i])\n","        else:\n","            print(\"%7s|\" % \"nan\")\n","\n","    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n","    for i in range(matrix.shape[0]):\n","        print(\"%s|\" % ('-' * 7), end='')\n","    print(\"%s|\" % ('-' * len(PRECISION)))\n","    print(\"|%s|\" % ('Recall'), end='')\n","\n","    for i in range(matrix.shape[0]):\n","        if recalllist[i] != -1:\n","            print(\"%1.5f|\" % (recalllist[i]), end='')\n","        else:\n","            print(\"%7s|\" % \"nan\", end='')\n","\n","    print('%s|' % (' ' * len(PRECISION)))\n","\n","\n","def getPrecisionRecall(cmatrix, label=1):\n","    trueP = cmatrix[label][label]\n","    denom = np.sum(cmatrix, axis=0)[label]\n","    if denom == 0:\n","        denom = 1\n","    recall = trueP / denom\n","    denom = np.sum(cmatrix, axis=1)[label]\n","    if denom == 0:\n","        denom = 1\n","    precision = trueP / denom\n","    return precision, recall\n","\n","\n","def getMacroPrecisionRecall(cmatrix):\n","    # TP + FP\n","    precisionlist = np.sum(cmatrix, axis=1)\n","    # TP + FN\n","    recalllist = np.sum(cmatrix, axis=0)\n","    precisionlist__ = [cmatrix[i][i] / x if x !=\n","                       0 else 0 for i, x in enumerate(precisionlist)]\n","    recalllist__ = [cmatrix[i][i] / x if x !=\n","                    0 else 0 for i, x in enumerate(recalllist)]\n","    precision = np.sum(precisionlist__)\n","    precision /= len(precisionlist__)\n","    recall = np.sum(recalllist__)\n","    recall /= len(recalllist__)\n","    return precision, recall\n","\n","\n","def getMicroPrecisionRecall(cmatrix):\n","    # TP + FP\n","    precisionlist = np.sum(cmatrix, axis=1)\n","    # TP + FN\n","    recalllist = np.sum(cmatrix, axis=0)\n","    num = 0.0\n","    for i in range(len(cmatrix)):\n","        num += cmatrix[i][i]\n","\n","    precision = num / np.sum(precisionlist)\n","    recall = num / np.sum(recalllist)\n","    return precision, recall\n","\n","\n","def getMacroMicroFScore(cmatrix):\n","    '''\n","    Returns macro and micro f-scores.\n","    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n","    '''\n","    precisionlist = np.sum(cmatrix, axis=1)\n","    recalllist = np.sum(cmatrix, axis=0)\n","    precisionlist__ = [cmatrix[i][i] / x if x !=\n","                       0 else 0 for i, x in enumerate(precisionlist)]\n","    recalllist__ = [cmatrix[i][i] / x if x !=\n","                    0 else 0 for i, x in enumerate(recalllist)]\n","    macro = 0.0\n","    for i in range(len(precisionlist)):\n","        denom = precisionlist__[i] + recalllist__[i]\n","        numer = precisionlist__[i] * recalllist__[i] * 2\n","        if denom == 0:\n","            denom = 1\n","        macro += numer / denom\n","    macro /= len(precisionlist)\n","\n","    num = 0.0\n","    for i in range(len(precisionlist)):\n","        num += cmatrix[i][i]\n","\n","    denom1 = np.sum(precisionlist)\n","    denom2 = np.sum(recalllist)\n","    pi = num / denom1\n","    rho = num / denom2\n","    denom = pi + rho\n","    if denom == 0:\n","        denom = 1\n","    micro = 2 * pi * rho / denom\n","    return macro, micro\n","\n","\n","class GraphManager:\n","    '''\n","    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n","    though is general enough to be useful otherwise as well.\n","    '''\n","\n","    def __init__(self):\n","        pass\n","\n","    def checkpointModel(self, saver, sess, modelPrefix,\n","                        globalStep=1000, redirFile=None):\n","        saver.save(sess, modelPrefix, global_step=globalStep)\n","        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n","              file=redirFile)\n","\n","    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n","                       redirFile=None):\n","        metaname = modelPrefix + '-%d.meta' % globalStep\n","        basename = os.path.basename(metaname)\n","        fileList = os.listdir(os.path.dirname(modelPrefix))\n","        fileList = [x for x in fileList if x.startswith(basename)]\n","        assert len(fileList) > 0, 'Checkpoint file not found'\n","        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n","        assert len(fileList) is 1, msg\n","        chkpt = basename + '/' + fileList[0]\n","        saver = tf.train.import_meta_graph(metaname)\n","        metaname = metaname[:-5]\n","        saver.restore(sess, metaname)\n","        graph = tf.get_default_graph()\n","        return graph"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usCaGutdHWDk","colab_type":"text"},"source":["# Bonsai.py"]},{"cell_type":"code","metadata":{"id":"-jpAffm_HWDo","colab_type":"code","colab":{}},"source":["# Copyright (c) Microsoft Corporation. All rights reserved.\n","# Licensed under the MIT license.\n","import warnings\n","\n","\n","class Bonsai:\n","    def __init__(self, numClasses, dataDimension, projectionDimension,\n","                 treeDepth, sigma,\n","                 isRegression=False, W=None, T=None, V=None, Z=None):\n","        '''\n","        Expected Dimensions:\n","\n","        Bonsai Params // Optional\n","        W [numClasses*totalNodes, projectionDimension]\n","        V [numClasses*totalNodes, projectionDimension]\n","        Z [projectionDimension, dataDimension + 1]\n","        T [internalNodes, projectionDimension]\n","\n","        internalNodes = 2**treeDepth - 1\n","        totalNodes = 2*internalNodes + 1\n","\n","        sigma - tanh non-linearity\n","        sigmaI - Indicator function for node probabilities\n","        sigmaI - has to be set to infinity(1e9 for practicality)\n","        while doing testing/inference\n","        numClasses will be reset to 1 in binary case\n","        '''\n","        self.dataDimension = dataDimension\n","        self.projectionDimension = projectionDimension\n","        self.isRegression = isRegression\n","\n","        if ((self.isRegression == True) & (numClasses != 1)):\n","            warnings.warn(\"Number of classes cannot be greater than 1 for regression\")\n","            self.numClasses = 1\n","\n","        if numClasses == 2:\n","            self.numClasses = 1\n","        else:\n","            self.numClasses = numClasses\n","\n","        self.treeDepth = treeDepth\n","        self.sigma = sigma\n","\n","        self.internalNodes = 2**self.treeDepth - 1\n","        self.totalNodes = 2 * self.internalNodes + 1\n","\n","        self.W = self.initW(W)\n","        self.V = self.initV(V)\n","        self.T = self.initT(T)\n","        self.Z = self.initZ(Z)\n","\n","        self.assertInit()\n","\n","        self.score = None\n","        self.X_ = None\n","        self.prediction = None\n","\n","    def initZ(self, Z):\n","        if Z is None:\n","            Z = tf.random_normal(\n","                [self.projectionDimension, self.dataDimension])\n","        Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n","        return Z\n","\n","    def initW(self, W):\n","        if W is None:\n","            W = tf.random_normal(\n","                [self.numClasses * self.totalNodes, self.projectionDimension])\n","        W = tf.Variable(W, name='W', dtype=tf.float32)\n","        return W\n","\n","    def initV(self, V):\n","        if V is None:\n","            V = tf.random_normal(\n","                [self.numClasses * self.totalNodes, self.projectionDimension])\n","        V = tf.Variable(V, name='V', dtype=tf.float32)\n","        return V\n","\n","    def initT(self, T):\n","        if T is None:\n","            T = tf.random_normal(\n","                [self.internalNodes, self.projectionDimension])\n","        T = tf.Variable(T, name='T', dtype=tf.float32)\n","        return T\n","\n","    def __call__(self, X, sigmaI):\n","        '''\n","        Function to build the Bonsai Tree graph\n","        Expected Dimensions\n","\n","        X is [_, self.dataDimension]\n","        '''\n","        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n","        assert (len(X.shape) == 2 and int(\n","            X.shape[1]) == self.dataDimension), errmsg\n","        if self.score is not None:\n","            return self.score, self.X_\n","\n","        X_ = tf.divide(tf.matmul(self.Z, X, transpose_b=True),\n","                       self.projectionDimension)\n","\n","        W_ = self.W[0:(self.numClasses)]\n","        V_ = self.V[0:(self.numClasses)]\n","\n","        self.__nodeProb = []\n","        self.__nodeProb.append(1)\n","\n","        score_ = self.__nodeProb[0] * tf.multiply(\n","            tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_)))\n","        for i in range(1, self.totalNodes):\n","            W_ = self.W[i * self.numClasses:((i + 1) * self.numClasses)]\n","            V_ = self.V[i * self.numClasses:((i + 1) * self.numClasses)]\n","\n","            T_ = tf.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],\n","                            [-1, self.projectionDimension])\n","            prob = (1 + ((-1)**(i + 1)) *\n","                    tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_))))\n","\n","            prob = tf.divide(prob, 2.0)\n","            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob\n","            self.__nodeProb.append(prob)\n","            score_ += self.__nodeProb[i] * tf.multiply(\n","                tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_)))\n","\n","        self.score = score_\n","        self.X_ = X_\n","        return self.score, self.X_\n","\n","    def getPrediction(self):\n","        '''\n","        Takes in a score tensor and outputs a integer class for each data point\n","        '''\n","\n","        # Classification.\n","        if (self.isRegression == False):\n","            if self.prediction is not None:\n","                return self.prediction\n","\n","            if self.numClasses > 2:\n","                self.prediction = tf.argmax(tf.transpose(self.score), 1)\n","            else:\n","                self.prediction = tf.argmax(\n","                    tf.concat([tf.transpose(self.score),\n","                               0 * tf.transpose(self.score)], 1), 1)\n","        # Regression.\n","        elif (self.isRegression == True):\n","            # For regression , scores are the actual predictions, just return them.\n","            self.prediction = self.score    \n","        return self.prediction\n","\n","    def assertInit(self):\n","        errmsg = \"Number of Classes for regression can only be 1.\"\n","        if (self.isRegression == True):\n","            assert (self.numClasses == 1), errmsg\n","        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n","        assert len(self.W.shape) == len(self.Z.shape), errRank\n","        assert len(self.W.shape) == len(self.T.shape), errRank\n","        assert len(self.W.shape) == 2, errRank\n","        msg = \"W and V should be of same Dimensions\"\n","        assert self.W.shape == self.V.shape, msg\n","        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n","        assert self.W.shape[0] == self.numClasses * self.totalNodes, errW\n","        assert self.W.shape[1] == self.projectionDimension, errW\n","        errZ = \"Z is [projectionDimension, dataDimension]\"\n","        assert self.Z.shape[0] == self.projectionDimension, errZ\n","        assert self.Z.shape[1] == self.dataDimension, errZ\n","        errT = \"T is [internalNodes, projectionDimension]\"\n","        assert self.T.shape[0] == self.internalNodes, errT\n","        assert self.T.shape[1] == self.projectionDimension, errT\n","        assert int(self.numClasses) > 0, \"numClasses should be > 1\"\n","        msg = \"# of features in data should be > 0\"\n","        assert int(self.dataDimension) > 0, msg\n","        msg = \"Projection should be  > 0 dims\"\n","        assert int(self.projectionDimension) > 0, msg\n","        msg = \"treeDepth should be >= 0\"\n","        assert int(self.treeDepth) >= 0, msg\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32LpTZCDHWDw","colab_type":"text"},"source":["# bonsaiTrainer.py"]},{"cell_type":"code","metadata":{"id":"TjW-WXGuHWDy","colab_type":"code","colab":{}},"source":["# Copyright (c) Microsoft Corporation. All rights reserved.\n","# Licensed under the MIT license.\n","\n","from __future__ import print_function\n","# import edgeml.utils as utils\n","\n","\n","class BonsaiTrainer:\n","    def __init__(self, bonsaiObj, lW, lT, lV, lZ, sW, sT, sV, sZ,\n","                 learningRate, X, Y, useMCHLoss=False, outFile=None, regLoss='huber'):\n","        '''\n","        bonsaiObj - Initialised Bonsai Object and Graph\n","        lW, lT, lV and lZ are regularisers to Bonsai Params\n","        sW, sT, sV and sZ are sparsity factors to Bonsai Params\n","        learningRate - learningRate fro optimizer\n","        X is the Data Placeholder - Dims [_, dataDimension]\n","        Y - Label placeholder for loss computation\n","        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n","        useMCHLoss - True - MultiClass - multiClassHingeLoss\n","        useMCHLoss - False - MultiClass - crossEntropyLoss\n","        '''\n","        self.testAcc1 = 0\n","        self.testLoss1 = 0\n","        self.regTestLoss1= 0\n","        self.pred1 = 0\n","        \n","        self.bonsaiObj = bonsaiObj\n","        self.regressionLoss = regLoss\n","\n","        self.lW = lW\n","        self.lV = lV\n","        self.lT = lT\n","        self.lZ = lZ\n","\n","        self.sW = sW\n","        self.sV = sV\n","        self.sT = sT\n","        self.sZ = sZ\n","\n","        self.Y = Y\n","        self.X = X\n","\n","        self.useMCHLoss = useMCHLoss\n","\n","        if outFile is not None:\n","            print(\"Outfile : \", outFile)\n","            self.outFile = open(outFile, 'w')\n","        else:\n","            self.outFile = sys.stdout\n","\n","        self.learningRate = learningRate\n","\n","        self.assertInit()\n","\n","        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n","\n","        self.score, self.X_ = self.bonsaiObj(self.X, self.sigmaI)\n","\n","        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n","\n","        self.trainStep = self.trainGraph()\n","        '''\n","        self.accuracy -> 'MAE' for Regression.\n","        self.accuracy -> 'Accuracy' for Classification.\n","        '''\n","        self.accuracy = self.accuracyGraph()\n","        self.prediction = self.bonsaiObj.getPrediction()\n","\n","        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n","            self.isDenseTraining = True\n","        else:\n","            self.isDenseTraining = False\n","\n","        self.hardThrsd()\n","        self.sparseTraining()\n","\n","    def lossGraph(self):\n","        '''\n","        Loss Graph for given Bonsai Obj\n","        '''\n","        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.bonsaiObj.Z)) +\n","                              self.lW * tf.square(tf.norm(self.bonsaiObj.W)) +\n","                              self.lV * tf.square(tf.norm(self.bonsaiObj.V)) +\n","                              self.lT * tf.square(tf.norm(self.bonsaiObj.T)))\n","\n","        # Loss functions for classification.\n","        if (self.bonsaiObj.isRegression == False):\n","            if (self.bonsaiObj.numClasses > 2):\n","                if self.useMCHLoss is True:\n","                    self.batch_th = tf.placeholder(tf.int64, name='batch_th')\n","                    self.marginLoss = utils.multiClassHingeLoss(\n","                        tf.transpose(self.score), self.Y,\n","                        self.batch_th)\n","                else:\n","                    self.marginLoss = utils.crossEntropyLoss(\n","                        tf.transpose(self.score), self.Y)\n","                self.loss = self.marginLoss + self.regLoss\n","            else:\n","                self.marginLoss = tf.reduce_mean(tf.nn.relu(\n","                    1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n","                self.loss = self.marginLoss + self.regLoss\n","\n","        # Loss functions for regression.\n","        elif (self.bonsaiObj.isRegression == True):\n","            if(self.regressionLoss == 'huber'):\n","                # Use of Huber Loss , because it is more robust to outliers.\n","                self.marginLoss = tf.losses.huber_loss(self.Y, tf.transpose(self.score))\n","                self.loss = self.marginLoss + self.regLoss\n","            elif (self.regressionLoss == 'l2'):\n","                # L2 loss function.\n","                self.marginLoss = tf.nn.l2_loss(self.Y - tf.transpose(self.score))\n","                self.loss = self.marginLoss + self.regLoss\n","\n","        return self.loss, self.marginLoss, self.regLoss\n","\n","    def trainGraph(self):\n","        '''\n","        Train Graph for the loss generated by Bonsai\n","        '''\n","        self.bonsaiObj.TrainStep = tf.train.AdamOptimizer(\n","            self.learningRate).minimize(self.loss)\n","\n","        return self.bonsaiObj.TrainStep\n","\n","    def accuracyGraph(self):\n","        '''\n","        Accuracy Graph to evaluate accuracy when needed\n","        '''\n","        if(self.bonsaiObj.isRegression == False):\n","            if (self.bonsaiObj.numClasses > 2):\n","                correctPrediction = tf.equal(\n","                    tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n","                self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n","            else:\n","                y_ = self.Y * 2 - 1\n","                correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n","                correctPrediction = tf.nn.relu(correctPrediction)\n","                correctPrediction = tf.ceil(tf.tanh(correctPrediction))\n","                self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n","\n","        elif (self.bonsaiObj.isRegression == True):\n","            # Accuracy for regression , in terms of mean absolute error.\n","            self.accuracy = mean_absolute_error(tf.reshape(\n","                self.score, [-1, 1]), tf.reshape(self.Y, [-1, 1]))\n","        return self.accuracy\n","\n","    def hardThrsd(self):\n","        '''\n","        Set up for hard Thresholding Functionality\n","        '''\n","        self.__Wth = tf.placeholder(tf.float32, name='Wth')\n","        self.__Vth = tf.placeholder(tf.float32, name='Vth')\n","        self.__Zth = tf.placeholder(tf.float32, name='Zth')\n","        self.__Tth = tf.placeholder(tf.float32, name='Tth')\n","\n","        self.__Woph = self.bonsaiObj.W.assign(self.__Wth)\n","        self.__Voph = self.bonsaiObj.V.assign(self.__Vth)\n","        self.__Toph = self.bonsaiObj.T.assign(self.__Tth)\n","        self.__Zoph = self.bonsaiObj.Z.assign(self.__Zth)\n","\n","        self.hardThresholdGroup = tf.group(\n","            self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n","\n","    def sparseTraining(self):\n","        '''\n","        Set up for Sparse Retraining Functionality\n","        '''\n","        self.__Wops = self.bonsaiObj.W.assign(self.__Wth)\n","        self.__Vops = self.bonsaiObj.V.assign(self.__Vth)\n","        self.__Zops = self.bonsaiObj.Z.assign(self.__Zth)\n","        self.__Tops = self.bonsaiObj.T.assign(self.__Tth)\n","\n","        self.sparseRetrainGroup = tf.group(\n","            self.__Wops, self.__Vops, self.__Tops, self.__Zops)\n","\n","    def runHardThrsd(self, sess):\n","        '''\n","        Function to run the IHT routine on Bonsai Obj\n","        '''\n","        currW = self.bonsaiObj.W.eval()\n","        currV = self.bonsaiObj.V.eval()\n","        currZ = self.bonsaiObj.Z.eval()\n","        currT = self.bonsaiObj.T.eval()\n","\n","        self.__thrsdW = hardThreshold(currW, self.sW)\n","        self.__thrsdV = hardThreshold(currV, self.sV)\n","        self.__thrsdZ = hardThreshold(currZ, self.sZ)\n","        self.__thrsdT = hardThreshold(currT, self.sT)\n","\n","        fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n","                    self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n","        sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n","\n","    def runSparseTraining(self, sess):\n","        '''\n","        Function to run the Sparse Retraining routine on Bonsai Obj\n","        '''\n","        currW = self.bonsaiObj.W.eval()\n","        currV = self.bonsaiObj.V.eval()\n","        currZ = self.bonsaiObj.Z.eval()\n","        currT = self.bonsaiObj.T.eval()\n","\n","        newW = copySupport(self.__thrsdW, currW)\n","        newV = copySupport(self.__thrsdV, currV)\n","        newZ = copySupport(self.__thrsdZ, currZ)\n","        newT = copySupport(self.__thrsdT, currT)\n","\n","        fd_st = {self.__Wth: newW, self.__Vth: newV,\n","                 self.__Zth: newZ, self.__Tth: newT}\n","        sess.run(self.sparseRetrainGroup, feed_dict=fd_st)\n","\n","    def assertInit(self):\n","        err = \"sparsity must be between 0 and 1\"\n","        assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n","        assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n","        assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n","        assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n","        errMsg = \"Dimension Mismatch, Y has to be [_, \" + \\\n","            str(self.bonsaiObj.numClasses) + \"]\"\n","        errCont = \" numClasses are 1 in case of Binary case by design\"\n","        assert (len(self.Y.shape) == 2 and\n","                self.Y.shape[1] == self.bonsaiObj.numClasses), errMsg + errCont\n","\n","    def saveParams(self, currDir):\n","        '''\n","        Function to save Parameter matrices into a given folder\n","        '''\n","        paramDir = currDir + '/'\n","        np.save(paramDir + \"W.npy\", self.bonsaiObj.W.eval())\n","        np.save(paramDir + \"V.npy\", self.bonsaiObj.V.eval())\n","        np.save(paramDir + \"T.npy\", self.bonsaiObj.T.eval())\n","        np.save(paramDir + \"Z.npy\", self.bonsaiObj.Z.eval())\n","        hyperParamDict = {'dataDim': self.bonsaiObj.dataDimension,\n","                          'projDim': self.bonsaiObj.projectionDimension,\n","                          'numClasses': self.bonsaiObj.numClasses,\n","                          'depth': self.bonsaiObj.treeDepth,\n","                          'sigma': self.bonsaiObj.sigma}\n","        hyperParamFile = paramDir + 'hyperParam.npy'\n","        np.save(hyperParamFile, hyperParamDict)\n","\n","    def loadModel(self, currDir):\n","        '''\n","        Load the Saved model and load it to the model using constructor\n","        Returns two dict one for params and other for hyperParams\n","        '''\n","        paramDir = currDir + '/'\n","        paramDict = {}\n","        paramDict['W'] = np.load(paramDir + \"W.npy\")\n","        paramDict['V'] = np.load(paramDir + \"V.npy\")\n","        paramDict['T'] = np.load(paramDir + \"T.npy\")\n","        paramDict['Z'] = np.load(paramDir + \"Z.npy\")\n","        hyperParamDict = np.load(paramDir + \"hyperParam.npy\").item()\n","        return paramDict, hyperParamDict\n","\n","    # Function to get aimed model size\n","    def getModelSize(self):\n","        '''\n","        Function to get aimed model size\n","        '''\n","        nnzZ, sizeZ, sparseZ = countnnZ(self.bonsaiObj.Z, self.sZ)\n","        nnzW, sizeW, sparseW = countnnZ(self.bonsaiObj.W, self.sW)\n","        nnzV, sizeV, sparseV = countnnZ(self.bonsaiObj.V, self.sV)\n","        nnzT, sizeT, sparseT = countnnZ(self.bonsaiObj.T, self.sT)\n","\n","        totalnnZ = (nnzZ + nnzT + nnzV + nnzW)\n","        totalSize = (sizeZ + sizeW + sizeV + sizeT)\n","        hasSparse = (sparseW or sparseV or sparseT or sparseZ)\n","        return totalnnZ, totalSize, hasSparse\n","    \n","    \n","    def train(self, batchSize, totalEpochs, sess,\n","              Xtrain, Xtest, Ytrain, Ytest, dataDir, currDir):\n","        '''\n","        The Dense - IHT - Sparse Retrain Routine for Bonsai Training\n","        '''\n","        resultFile = open(dataDir + '/TFBonsaiResults.txt', 'a+')\n","        numIters = Xtrain.shape[0] / batchSize\n","\n","        totalBatches = numIters * totalEpochs\n","\n","        bonsaiObjSigmaI = 1\n","\n","        counter = 0\n","        if self.bonsaiObj.numClasses > 2:\n","            trimlevel = 15\n","        else:\n","            trimlevel = 5\n","        ihtDone = 0\n","        if (self.bonsaiObj.isRegression == True):\n","            maxTestAcc = 100000007\n","        else:\n","            maxTestAcc = -10000\n","        if self.isDenseTraining is True:\n","            ihtDone = 1\n","            bonsaiObjSigmaI = 1\n","            itersInPhase = 0\n","\n","        header = '*' * 20\n","        for i in range(totalEpochs):\n","            print(\"\\nEpoch Number: \" + str(i), file=self.outFile)\n","\n","            '''\n","            trainAcc -> For Regression, it is 'Mean Absolute Error'.\n","            trainAcc -> For Classification, it is 'Accuracy'.\n","            '''\n","            trainAcc = 0.0\n","            trainLoss = 0.0\n","\n","            numIters = int(numIters)\n","            for j in range(numIters):\n","\n","                if counter == 0:\n","                    msg = \" Dense Training Phase Started \"\n","                    print(\"\\n%s%s%s\\n\" %\n","                          (header, msg, header), file=self.outFile)\n","\n","                # Updating the indicator sigma\n","                if ((counter == 0) or (counter == int(totalBatches / 3.0)) or\n","                        (counter == int(2 * totalBatches / 3.0))) and (self.isDenseTraining is False):\n","                    bonsaiObjSigmaI = 1\n","                    itersInPhase = 0\n","\n","                elif (itersInPhase % 100 == 0):\n","                    indices = np.random.choice(Xtrain.shape[0], 100)\n","                    batchX = Xtrain[indices, :]\n","                    batchY = Ytrain[indices, :]\n","                    batchY = np.reshape(\n","                        batchY, [-1, self.bonsaiObj.numClasses])\n","\n","                    _feed_dict = {self.X: batchX}\n","                    Xcapeval = self.X_.eval(feed_dict=_feed_dict)\n","                    Teval = self.bonsaiObj.T.eval()\n","\n","                    sum_tr = 0.0\n","                    for k in range(0, self.bonsaiObj.internalNodes):\n","                        sum_tr += (np.sum(np.abs(np.dot(Teval[k], Xcapeval))))\n","\n","                    if(self.bonsaiObj.internalNodes > 0):\n","                        sum_tr /= (100 * self.bonsaiObj.internalNodes)\n","                        sum_tr = 0.1 / sum_tr\n","                    else:\n","                        sum_tr = 0.1\n","                    sum_tr = min(\n","                        1000, sum_tr * (2**(float(itersInPhase) /\n","                                            (float(totalBatches) / 30.0))))\n","\n","                    bonsaiObjSigmaI = sum_tr\n","\n","                itersInPhase += 1\n","                batchX = Xtrain[j * batchSize:(j + 1) * batchSize]\n","                batchY = Ytrain[j * batchSize:(j + 1) * batchSize]\n","                batchY = np.reshape(\n","                    batchY, [-1, self.bonsaiObj.numClasses])\n","\n","                if self.bonsaiObj.numClasses > 2:\n","                    if self.useMCHLoss is True:\n","                        _feed_dict = {self.X: batchX, self.Y: batchY,\n","                                      self.batch_th: batchY.shape[0],\n","                                      self.sigmaI: bonsaiObjSigmaI}\n","                    else:\n","                        _feed_dict = {self.X: batchX, self.Y: batchY,\n","                                      self.sigmaI: bonsaiObjSigmaI}\n","                else:\n","                    _feed_dict = {self.X: batchX, self.Y: batchY,\n","                                  self.sigmaI: bonsaiObjSigmaI}\n","\n","                # Mini-batch training\n","                _, batchLoss, batchAcc = sess.run(\n","                    [self.trainStep, self.loss, self.accuracy],\n","                    feed_dict=_feed_dict)\n","\n","                # Classification.\n","                if (self.bonsaiObj.isRegression == False):\n","                    trainAcc += batchAcc\n","                    trainLoss += batchLoss\n","                # Regression.\n","                else:\n","                    trainAcc += np.mean(batchAcc)\n","                    trainLoss += np.mean(batchLoss)\n","\n","                # Training routine involving IHT and sparse retraining\n","                if (counter >= int(totalBatches / 3.0) and\n","                    (counter < int(2 * totalBatches / 3.0)) and\n","                    counter % trimlevel == 0 and\n","                        self.isDenseTraining is False):\n","                    self.runHardThrsd(sess)\n","                    if ihtDone == 0:\n","                        msg = \" IHT Phase Started \"\n","                        print(\"\\n%s%s%s\\n\" %\n","                              (header, msg, header), file=self.outFile)\n","                    ihtDone = 1\n","                elif ((ihtDone == 1 and counter >= int(totalBatches / 3.0) and\n","                       (counter < int(2 * totalBatches / 3.0)) and\n","                       counter % trimlevel != 0 and\n","                       self.isDenseTraining is False) or\n","                        (counter >= int(2 * totalBatches / 3.0) and\n","                            self.isDenseTraining is False)):\n","                    self.runSparseTraining(sess)\n","                    if counter == int(2 * totalBatches / 3.0):\n","                        msg = \" Sparse Retraining Phase Started \"\n","                        print(\"\\n%s%s%s\\n\" %\n","                              (header, msg, header), file=self.outFile)\n","                counter += 1\n","            try:\n","                if (self.bonsaiObj.isRegression == True):\n","                    print(\"\\nRegression Train Loss: \" + str(trainLoss / numIters) +\n","                          \"\\nTraining MAE (Regression): \" + str(trainAcc / numIters),\n","                          file=self.outFile)\n","                else:\n","                    print(\"\\nClassification Train Loss: \" + str(trainLoss / numIters) +\n","                          \"\\nTraining accuracy (Classification): \" + str(trainAcc / numIters),\n","                          file=self.outFile)\n","            except:\n","                continue\n","\n","            oldSigmaI = bonsaiObjSigmaI\n","            bonsaiObjSigmaI = 1e9\n","\n","            if self.bonsaiObj.numClasses > 2:\n","                if self.useMCHLoss is True:\n","                    _feed_dict = {self.X: Xtest, self.Y: Ytest,\n","                                  self.batch_th: Ytest.shape[0],\n","                                  self.sigmaI: bonsaiObjSigmaI}\n","                else:\n","                    _feed_dict = {self.X: Xtest, self.Y: Ytest,\n","                                  self.sigmaI: bonsaiObjSigmaI}\n","            else:\n","                _feed_dict = {self.X: Xtest, self.Y: Ytest,\n","                              self.sigmaI: bonsaiObjSigmaI}\n","\n","            # This helps in direct testing instead of extracting the model out\n","\n","            testAcc, testLoss, regTestLoss, pred = sess.run(\n","                [self.accuracy, self.loss, self.regLoss, self.prediction], feed_dict=_feed_dict)\n","            self.testAcc1, self.testLoss1, self.regTestLoss1, self.pred1 = sess.run(\n","                [self.accuracy, self.loss, self.regLoss, self.prediction], feed_dict=_feed_dict)\n","\n","            if ihtDone == 0:\n","                if (self.bonsaiObj.isRegression == False):\n","                    maxTestAcc = -10000\n","                    maxTestAccEpoch = i\n","                elif (self.bonsaiObj.isRegression == True):\n","                    maxTestAcc = testAcc\n","                    maxTestAccEpoch = i\n","\n","            else:\n","                if (self.bonsaiObj.isRegression == False):\n","                    if maxTestAcc <= testAcc:\n","                        maxTestAccEpoch = i\n","                        maxTestAcc = testAcc\n","                        self.saveParams(currDir)\n","                elif (self.bonsaiObj.isRegression == True):\n","                    print(\"Minimum Training MAE : \", np.mean(maxTestAcc))\n","                    if maxTestAcc >= testAcc:\n","                        # For regression , we're more interested in the minimum MAE.\n","                        maxTestAccEpoch = i\n","                        maxTestAcc = testAcc\n","                        self.saveParams(currDir)\n","\n","            if (self.bonsaiObj.isRegression == True):\n","                print(\"Testing MAE %g\" % np.mean(testAcc), file=self.outFile)\n","            else:\n","                print(\"Test accuracy %g\" % np.mean(testAcc), file=self.outFile)\n","#                 print(\"Prediction %g\" %pred ,file=self.outFile)\n","\n","            if (self.bonsaiObj.isRegression == True):\n","                testAcc = np.mean(testAcc)\n","            else:\n","                testAcc = testAcc\n","                maxTestAcc = maxTestAcc\n","\n","            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n","                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\",\n","                  file=self.outFile)\n","            self.outFile.flush()\n","\n","            bonsaiObjSigmaI = oldSigmaI\n","\n","        # sigmaI has to be set to infinity to ensure\n","        # only a single path is used in inference\n","        bonsaiObjSigmaI = 1e9\n","        print(\"\\nNon-Zero : \" + str(self.getModelSize()[0]) + \" Model Size: \" +\n","              str(float(self.getModelSize()[1]) / 1024.0) + \" KB hasSparse: \" +\n","              str(self.getModelSize()[2]) + \"\\n\", file=self.outFile)\n","\n","        if (self.bonsaiObj.isRegression == True):\n","            maxTestAcc = np.mean(maxTestAcc)\n","\n","        if (self.bonsaiObj.isRegression == True):\n","            print(\"For Regression, Minimum MAE at compressed\" +\n","                  \" model size(including early stopping): \" +\n","                  str(maxTestAcc) + \" at Epoch: \" +\n","                  str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n","                  \" MAE: \" + str(testAcc), file=self.outFile)\n","\n","            resultFile.write(\"MinTestMAE: \" + str(maxTestAcc) +\n","                             \" at Epoch(totalEpochs): \" +\n","                             str(maxTestAccEpoch + 1) +\n","                             \"(\" + str(totalEpochs) + \")\" + \" ModelSize: \" +\n","                             str(float(self.getModelSize()[1]) / 1024.0) +\n","                             \" KB hasSparse: \" + str(self.getModelSize()[2]) +\n","                             \" Param Directory: \" +\n","                             str(os.path.abspath(currDir)) + \"\\n\")\n","\n","        elif (self.bonsaiObj.isRegression == False):\n","            print(\"For Classification, Maximum Test accuracy at compressed\" +\n","                  \" model size(including early stopping): \" +\n","                  str(maxTestAcc) + \" at Epoch: \" +\n","                  str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n","                  \" Accuracy: \" + str(testAcc), file=self.outFile)\n","\n","            resultFile.write(\"MaxTestAcc: \" + str(maxTestAcc) +\n","                             \" at Epoch(totalEpochs): \" +\n","                             str(maxTestAccEpoch + 1) +\n","                             \"(\" + str(totalEpochs) + \")\" + \" ModelSize: \" +\n","                             str(float(self.getModelSize()[1]) / 1024.0) +\n","                             \" KB hasSparse: \" + str(self.getModelSize()[2]) +\n","                             \" Param Directory: \" +\n","                             str(os.path.abspath(currDir)) + \"\\n\")\n","        print(\"The Model Directory: \" + currDir + \"\\n\")\n","\n","        resultFile.close()\n","        self.outFile.flush()\n","\n","        if self.outFile is not sys.stdout:\n","            self.outFile.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r_CSgAOxJU8z","colab_type":"text"},"source":["# Obtain data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"i_QU1xakLjTm","colab":{}},"source":["\n","# os.chdir(\"/home/iot/Documents/dataset_fog_release/dataset/ML\")\n","# x_train = pd.read_csv(\"final_x.csv\")\n","# x_train.drop('Unnamed: 0',axis=1,inplace=True)\n","# y_list = pd.read_csv(\"final_y.csv\")\n","# y_list.drop('Unnamed: 0',axis=1,inplace=True)\n","# y_list = y_list-1\n","# x =pd.concat([x_train,y_list],axis=1)\n","# x= x[['0','mean1', 'mean2', 'mean9', 'rms2', 'abovemn2', 'belowmn2', 'soc4',\n","#        'PSD2', 'FI1', 'FI2', 'FI3', 'FI5', 'FI9', 'FImag2']]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"27pKEISHHWEK","colab_type":"code","outputId":"ca747bfd-360b-4dbb-9f8b-b242fd01f213","executionInfo":{"status":"ok","timestamp":1567659429748,"user_tz":-330,"elapsed":830,"user":{"displayName":"Gokul Hari","photoUrl":"","userId":"16159457985484250305"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive \n","drive.mount (\"/content/gdrive\")\n","\n","DATA_DIR = \"/content/gdrive/My Drive/Recovery/dataset_fog_release/dataset/DATA_DIR\"\n","windowLen = '2'\n","(dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest, mean, std) = preProcessData(DATA_DIR,windowLen, isRegression=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2_ZfsH8GHWEP","colab_type":"text"},"source":["# Model Parameters\n","\n","Note that Bonsai is designed for low-memory setting and the best results are obtained when operating in that setting. Use the sparsity, projection dimension and tree depth to vary the model size."]},{"cell_type":"code","metadata":{"id":"7ZMc-pm_HWEU","colab_type":"code","colab":{}},"source":["sigma = 1.0 #Sigmoid parameter for tanh\n","depth = 10 #Depth of Bonsai Tree\n","projectionDimension = 48 #Lower Dimensional space for Bonsai to work on\n","\n","#Regularizers for Bonsai Parameters\n","regZ = 0.0001\n","regW = 0.001\n","regV = 0.001\n","regT = 0.001\n","\n","totalEpochs = 100\n","\n","learningRate = 0.01\n","\n","outFile = None\n","\n","#Sparsity for Bonsai Parameters. x => 100*x % are non-zeros\n","sparZ = 0.2\n","sparW = 0.3\n","sparV = 0.3\n","sparT = 0.62\n","\n","batchSize = np.maximum(100, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n","\n","useMCHLoss = True #only for Multiclass cases True: Multiclass-Hing Loss, False: Cross Entropy. \n","\n","#Bonsai uses one classier for Binary, thus this condition\n","if numClasses == 2:\n","    numClasses = 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ob350Z_jHWEZ","colab_type":"text"},"source":["Placeholders for Data feeding during training and infernece"]},{"cell_type":"code","metadata":{"id":"-cDEwJXLHWEa","colab_type":"code","colab":{}},"source":["X = tf.placeholder(\"float32\", [None, dataDimension])\n","Y = tf.placeholder(\"float32\", [None, numClasses])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g9AWWX7SHWEh","colab_type":"text"},"source":["Creating a directory for current model in the datadirectory using timestamp"]},{"cell_type":"code","metadata":{"id":"QekJmbhZHWEk","colab_type":"code","colab":{}},"source":["currDir = createTimeStampDir(DATA_DIR)\n","dumpCommand(sys.argv, currDir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Si3zazw0HWEo","colab_type":"text"},"source":["# Bonsai Graph Object\n","\n","Instantiating the Bonsai Graph which will be used for training and inference."]},{"cell_type":"code","metadata":{"id":"X7336O8mHWEq","colab_type":"code","colab":{}},"source":["bonsaiObj = Bonsai(numClasses, dataDimension, projectionDimension, depth, sigma)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCqKPxJVHWEv","colab_type":"text"},"source":["# Bonsai Trainer Object\n","\n","Instantiating the Bonsai Trainer which will be used for 3 phase training."]},{"cell_type":"code","metadata":{"id":"TqjoQ9-lHWEw","colab_type":"code","colab":{}},"source":["bonsaiTrainer = BonsaiTrainer(bonsaiObj, regW, regT, regV, regZ, sparW, sparT, sparV, sparZ,\n","                              learningRate, X, Y, useMCHLoss, outFile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0b1xQdpdHWE4","colab_type":"code","colab":{}},"source":["sess = tf.InteractiveSession()\n","sess.run(tf.global_variables_initializer())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HtKGhLLHWE9","colab_type":"text"},"source":["# Bonsai Training Routine\n","\n","The method to to run the 3 phase training, followed by giving out the best early stopping model, accuracy along with saving of the parameters."]},{"cell_type":"code","metadata":{"id":"tDlywVhoHWE-","colab_type":"code","outputId":"0e248cbe-efa8-43d9-80e6-3d6ba57fc17b","colab":{"base_uri":"https://localhost:8080/","height":108}},"source":["bonsaiTrainer.train(batchSize, totalEpochs, sess,\n","                    Xtrain, Xtest, Ytrain, Ytest, DATA_DIR, currDir)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Epoch Number: 0\n","\n","******************** Dense Training Phase Started ********************\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HvAmO9DpHWFE","colab_type":"code","colab":{}},"source":["pred = bonsaiTrainer.pred1\n","y_pred=[]\n","for i in pred:\n","  if i == 0:\n","    y_pred.append(1)\n","  else:\n","    y_pred.append(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xEXln5MsM7or","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix,classification_report\n","# Ytest = np.argmax(Ytest,axis=1)\n","print (confusion_matrix(Ytest,y_pred))\n","print (classification_report(Ytest,y_pred,digits=5))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"icMCp5EQRfwR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}